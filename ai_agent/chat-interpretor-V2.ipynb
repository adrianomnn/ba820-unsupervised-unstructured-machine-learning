{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOS00kqykpwr3j/apEZ0JDn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"842ff32a6b7b4d0994f8390afc57dfe1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_59f37fe9e55244a6a1bb56f159446a4c","IPY_MODEL_5ccec492c72a47dbb5ace2c9f700464b","IPY_MODEL_5f6de6d01d944ed0afa1f7919b5e2211"],"layout":"IPY_MODEL_27ac636c0b40469d8174586ef2fd018a"}},"59f37fe9e55244a6a1bb56f159446a4c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52e37d7246ed4b2889f8e7ab2756f190","placeholder":"​","style":"IPY_MODEL_04e64863d24649568e3cbc17b9e42781","value":"Loading checkpoint shards: 100%"}},"5ccec492c72a47dbb5ace2c9f700464b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4469aad29a294a84b5a914616a54a8b4","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6b31e904894d4a8c800bd23814423408","value":5}},"5f6de6d01d944ed0afa1f7919b5e2211":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_593c22b87ee447528d15a160f6c653ef","placeholder":"​","style":"IPY_MODEL_42c1e40d28e24490a730f16314dc1ec5","value":" 5/5 [00:09&lt;00:00,  1.73s/it]"}},"27ac636c0b40469d8174586ef2fd018a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52e37d7246ed4b2889f8e7ab2756f190":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04e64863d24649568e3cbc17b9e42781":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4469aad29a294a84b5a914616a54a8b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b31e904894d4a8c800bd23814423408":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"593c22b87ee447528d15a160f6c653ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42c1e40d28e24490a730f16314dc1ec5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qXFt3CYoPXsU","executionInfo":{"status":"ok","timestamp":1740708250222,"user_tz":300,"elapsed":775,"user":{"displayName":"Ran Zhang","userId":"06161881150186675682"}},"outputId":"5da6f73e-775a-423b-e0e4-62ea204e5101"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Found 1 images in the folder.\n","line-sale.png\n"]}],"source":["from google.colab import drive\n","import glob, os\n","from PIL import Image\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Set the image folder path (adjust if needed)\n","image_folder = \"/content/drive/MyDrive/BA820-Unsupervised ML/BA 820 Team 6/milestone 2/MarketingCharts\"\n","\n","# Retrieve list of image files in the folder (adjust file pattern as needed)\n","image_paths = glob.glob(os.path.join(image_folder, \"*.*\"))\n","print(f\"Found {len(image_paths)} images in the folder.\")\n","for path in image_paths:\n","    print(os.path.basename(path))\n"]},{"cell_type":"code","source":["from google.colab import userdata\n","import openai\n","openai.api_key = userdata.get('openai')  # OpenAI API key for ChatGPT"],"metadata":{"id":"ensBZfwyQR6G","executionInfo":{"status":"ok","timestamp":1740708251141,"user_tz":300,"elapsed":920,"user":{"displayName":"Ran Zhang","userId":"06161881150186675682"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","# Set device and torch_dtype based on GPU availability\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","torch_dtype = torch.float16 if device==\"cuda\" else torch.float32"],"metadata":{"id":"yhSLSQJ5RKOA","executionInfo":{"status":"ok","timestamp":1740708253042,"user_tz":300,"elapsed":1896,"user":{"displayName":"Ran Zhang","userId":"06161881150186675682"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["chatgpt_model = \"gpt-4o-mini\"  # Adjust if needed"],"metadata":{"id":"MflqjyGFSQ9H","executionInfo":{"status":"ok","timestamp":1740708253046,"user_tz":300,"elapsed":2,"user":{"displayName":"Ran Zhang","userId":"06161881150186675682"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n","qwen_model_id = \"Qwen/Qwen2-VL-7B-Instruct\"\n","qwen_model = Qwen2VLForConditionalGeneration.from_pretrained(\n","    qwen_model_id,\n","    torch_dtype=torch_dtype,\n","    device_map=\"auto\"\n",")\n","qwen_processor = AutoProcessor.from_pretrained(qwen_model_id)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191,"referenced_widgets":["842ff32a6b7b4d0994f8390afc57dfe1","59f37fe9e55244a6a1bb56f159446a4c","5ccec492c72a47dbb5ace2c9f700464b","5f6de6d01d944ed0afa1f7919b5e2211","27ac636c0b40469d8174586ef2fd018a","52e37d7246ed4b2889f8e7ab2756f190","04e64863d24649568e3cbc17b9e42781","4469aad29a294a84b5a914616a54a8b4","6b31e904894d4a8c800bd23814423408","593c22b87ee447528d15a160f6c653ef","42c1e40d28e24490a730f16314dc1ec5"]},"id":"-ZioJrEMSUng","executionInfo":{"status":"ok","timestamp":1740708273010,"user_tz":300,"elapsed":19962,"user":{"displayName":"Ran Zhang","userId":"06161881150186675682"}},"outputId":"b4ae5116-181d-497c-d58f-0cf6b2b4652c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"842ff32a6b7b4d0994f8390afc57dfe1"}},"metadata":{}}]},{"cell_type":"code","source":["from transformers import AutoProcessor, AutoModelForCausalLM\n","florence_model_id = \"microsoft/Florence-2-large\"\n","florence_processor = AutoProcessor.from_pretrained(florence_model_id, trust_remote_code=True)\n","florence_model = AutoModelForCausalLM.from_pretrained(\n","    florence_model_id,\n","    torch_dtype=torch_dtype,\n","    trust_remote_code=True\n",").to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OgWTLx1XSa3g","executionInfo":{"status":"ok","timestamp":1740708287585,"user_tz":300,"elapsed":14572,"user":{"displayName":"Ran Zhang","userId":"06161881150186675682"}},"outputId":"a445df5b-84e1-4b99-f025-089041b02f81"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n","  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"]}]},{"cell_type":"code","source":["from openai import OpenAI\n","client = OpenAI(api_key=openai.api_key)  # Make sure your OpenAI API key is set\n","\n","def get_chatgpt_insight():\n","    # Simplified prompt for ChatGPT\n","    persona_note = \"Assume the user is a U.S.-based small business owner with at most some college education. Use plain, easy-to-understand English and avoid technical jargon.\"\n","    task_prompt = \"Describe the business insights and actions based on the chart.\"\n","    chat_messages = [\n","        {\"role\": \"system\", \"content\": persona_note},\n","        {\"role\": \"user\", \"content\": task_prompt}\n","    ]\n","    try:\n","        chat_response = client.chat.completions.create(\n","            model=chatgpt_model,\n","            messages=chat_messages\n","        )\n","        chat_text = chat_response.choices[0].message.content.strip()\n","    except Exception as e:\n","        chat_text = f\"Error: {e}\"\n","    return chat_text"],"metadata":{"id":"v1v3vEtRShE3","executionInfo":{"status":"ok","timestamp":1740708287603,"user_tz":300,"elapsed":12,"user":{"displayName":"Ran Zhang","userId":"06161881150186675682"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def extract_qwen_insights(image_path, prompt_text):\n","    try:\n","        image = Image.open(image_path)\n","        # Prepare conversation: image placeholder + prompt\n","        conversation = [{\n","            \"role\": \"user\",\n","            \"content\": [\n","                {\"type\": \"image\"},\n","                {\"type\": \"text\", \"text\": prompt_text}\n","            ]\n","        }]\n","        # Set the chat template for Qwen (pass prompt_text directly)\n","        text_input = qwen_processor.apply_chat_template(\n","            conversation,\n","            chat_template=prompt_text,\n","            add_generation_prompt=True\n","        )\n","        # Prepare model inputs\n","        inputs = qwen_processor(text=[text_input], images=[image], return_tensors=\"pt\", padding=True)\n","        inputs = inputs.to(qwen_model.device)\n","        # Generate output tokens\n","        output_ids = qwen_model.generate(**inputs, max_new_tokens=128)\n","        prompt_len = inputs.input_ids.shape[1]\n","        gen_tokens = output_ids[:, prompt_len:]\n","        generated_texts = qwen_processor.batch_decode(gen_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","        insight_text = generated_texts[0].strip() if generated_texts else \"No insights were generated for this image.\"\n","        return insight_text\n","    except Exception as e:\n","        print(f\"Error processing {image_path} in Qwen: {e}\")\n","        return \"Error: unable to process image\""],"metadata":{"id":"ifkPRE13Sl71","executionInfo":{"status":"ok","timestamp":1740708287622,"user_tz":300,"elapsed":16,"user":{"displayName":"Ran Zhang","userId":"06161881150186675682"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def run_florence_example(task_prompt, image, text_input=None):\n","    \"\"\"\n","    Generate insights for the image using the Florence model.\n","    \"\"\"\n","    if text_input is None:\n","        prompt = task_prompt\n","    else:\n","        prompt = task_prompt + text_input\n","\n","    image = image.convert('RGB')\n","    inputs = florence_processor(text=prompt, images=image, return_tensors=\"pt\")\n","\n","    # Only cast pixel_values to float16; keep input_ids as integers.\n","    for key, tensor in inputs.items():\n","        if key == \"pixel_values\":\n","            inputs[key] = tensor.to(device, dtype=torch.float16)\n","        else:\n","            inputs[key] = tensor.to(device)\n","\n","    generated_ids = florence_model.generate(\n","        input_ids=inputs[\"input_ids\"],\n","        pixel_values=inputs[\"pixel_values\"],\n","        max_new_tokens=1024,\n","        early_stopping=False,\n","        do_sample=False,\n","        num_beams=3,\n","    )\n","    prompt_len = inputs.input_ids.shape[1]\n","    gen_tokens = generated_ids[:, prompt_len:]\n","\n","    # For debugging: print raw output before post-processing\n","    raw_generated = florence_processor.batch_decode(gen_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)[0]\n","    print(\"Florence raw output:\", raw_generated)\n","\n","    parsed_answer = florence_processor.post_process_generation(\n","        raw_generated,\n","        task=task_prompt,\n","        image_size=(image.width, image.height)\n","    )\n","    return parsed_answer"],"metadata":{"id":"SJQJJ3TkSpA9","executionInfo":{"status":"ok","timestamp":1740708287642,"user_tz":300,"elapsed":14,"user":{"displayName":"Ran Zhang","userId":"06161881150186675682"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["results = []  # to accumulate results for each image\n","\n","# Use a simplified prompt for Qwen and Florence\n","full_prompt_text = \"Describe the business insights and actions based on the chart.\"\n","\n","for img_path in image_paths:\n","    image_name = os.path.basename(img_path)\n","    print(f\"\\nProcessing image: {image_name}\")\n","\n","    # --- ChatGPT Analysis ---\n","    chat_text = get_chatgpt_insight()\n","    print(\"ChatGPT Output:\\n\", chat_text)\n","\n","    # --- Qwen Analysis ---\n","    qwen_text = extract_qwen_insights(img_path, full_prompt_text)\n","    print(\"Qwen Output:\\n\", qwen_text)\n","\n","    # --- Florence Analysis ---\n","    image = Image.open(img_path)\n","    florence_text = run_florence_example(full_prompt_text, image)\n","    print(\"Florence Output:\\n\", florence_text)\n","\n","    # Save insights from all models\n","    results.append({\n","        \"image\": image_name,\n","        \"ChatGPT_Insight\": chat_text,\n","        \"Qwen_Insight\": qwen_text,\n","        \"Florence_Insight\": florence_text\n","    })\n","\n","    _ = input(\"Press Enter to continue to evaluation for this image...\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P6ZKGBKMSsOb","executionInfo":{"status":"ok","timestamp":1740708310531,"user_tz":300,"elapsed":22881,"user":{"displayName":"Ran Zhang","userId":"06161881150186675682"}},"outputId":"3ec007f5-17c8-4b1a-c825-0afe6ac8162d"},"execution_count":10,"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Processing image: line-sale.png\n","ChatGPT Output:\n"," Sure! I'll help you with that. However, since I can't see the chart you're referring to, I can guide you on how to analyze it. Here’s what to think about when looking at a chart for business insights:\n","\n","1. **Trends**: Look for patterns over time. Are there any noticeable increases or decreases? For example, if sales are going up, that’s a good sign. If they’re going down, you may need to investigate why.\n","\n","2. **Comparisons**: See how different months, products, or categories compare to each other. Which ones are performing better? This can help you know where to focus your efforts.\n","\n","3. **Outliers**: Check for any data points that are very different from the others. These could be errors or represent unique situations that are worth looking into.\n","\n","4. **Goals**: Compare your performance to your business goals. Are you where you want to be? If not, you may need to adjust your strategies.\n","\n","5. **Customer Behavior**: If the chart relates to customer actions (like purchases or website visits), think about what might have caused spikes or drops in activity. \n","\n","**Actions Based on Insights**:\n","\n","- **If sales are increasing**: Consider investing more in marketing or expanding your product line.\n","  \n","- **If sales are decreasing**: Investigate the cause. Maybe gather customer feedback or consider adjusting prices.\n","\n","- **If certain products sell better**: Focus on promoting those products more and maybe even discontinue less popular ones.\n","\n","- **If you see seasonal trends**: Prepare in advance for busy seasons by increasing stock or hiring seasonal help.\n","\n","- **If you identify a problem area**: Make a plan to address it, whether it’s improving customer service or enhancing your marketing efforts.\n","\n","Always keep in mind that regular review of your charts can help you stay on track and make informed decisions for your business. If you have specific data from the chart, feel free to share, and I can help further!\n","Error processing /content/drive/MyDrive/BA820-Unsupervised ML/BA 820 Team 6/milestone 2/MarketingCharts/line-sale.png in Qwen: Image features and image tokens do not match: tokens: 0, features 70\n","Qwen Output:\n"," Error: unable to process image\n","Florence raw output: \n","Florence Output:\n"," {'Describe the business insights and actions based on the chart.': ''}\n","Press Enter to continue to evaluation for this image...\n"]}]}]}